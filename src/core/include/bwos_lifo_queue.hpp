/*
 * Copyright (c) 2023 Maikel Nadolski
 * Copyright (c) 2023 NVIDIA Corporation
 *
 * Licensed under the Apache License Version 2.0 with LLVM Exceptions
 * (the "License"); you may not use this file except in compliance with
 * the License. You may obtain a copy of the License at
 *
 *   https://llvm.org/LICENSE.txt
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#pragma once

#include <atomic>
#include <bit>
#include <cassert>
#include <cstddef>
#include <cstdint>
#include <memory>
#include <utility>
#include <vector>

/**
 * This is an implementation of the BWOS queue as described in
 * BWoS: Formally Verified Block-based Work Stealing for Parallel Processing (Wang et al. 2023)
 */

namespace cw::bwos {

inline constexpr std::size_t hardware_destructive_interference_size = 64;
inline constexpr std::size_t hardware_constructive_interference_size = 64;

enum class lifo_queue_error_code {
  success,
  done,
  empty,
  full,
  conflict,
};

template <class Tp> struct fetch_result {
  lifo_queue_error_code status;
  Tp value;
};

template <class Tp, class Allocator = std::allocator<Tp>> class lifo_queue {
public:
  explicit lifo_queue(std::size_t num_blocks, std::size_t block_size,
                      Allocator allocator = Allocator());

  auto pop_back() noexcept -> Tp;

  auto steal_front() noexcept -> Tp;

  auto push_back(Tp value) noexcept -> bool;

  template <class Iterator, class Sentinel>
  auto push_back(Iterator first, Sentinel last) noexcept -> Iterator;

  [[nodiscard]]
  auto block_size() const noexcept -> std::size_t;
  [[nodiscard]]
  auto num_blocks() const noexcept -> std::size_t;

private:
  template <class Sp>
  using allocator_of_t = std::allocator_traits<Allocator>::template rebind_alloc<Sp>;

  struct block_type {
    explicit block_type(std::size_t block_size, Allocator allocator = Allocator());

    block_type(const block_type&);
    auto operator=(const block_type&) -> block_type&;

    block_type(block_type&&) noexcept;
    auto operator=(block_type&&) noexcept -> block_type&;

    auto put(Tp value) noexcept -> lifo_queue_error_code;

    template <class Iterator, class Sentinel>
    auto bulk_put(Iterator first, Sentinel last) noexcept -> Iterator;

    auto get() noexcept -> fetch_result<Tp>;

    auto steal(std::uint32_t round) noexcept -> fetch_result<Tp>;

    auto takeover() noexcept -> void;

    [[nodiscard]]
    auto is_writable(std::uint32_t round) const noexcept -> bool;

    void grant() noexcept;

    auto reclaim(std::uint32_t round) noexcept -> void;

    [[nodiscard]]
    auto block_size() const noexcept -> std::size_t;

    auto reduce_round() noexcept -> void;

    alignas(hardware_destructive_interference_size) std::atomic<std::uint64_t> head_{};
    alignas(hardware_destructive_interference_size) std::atomic<std::uint64_t> tail_{};
    alignas(hardware_destructive_interference_size) std::atomic<std::uint64_t> steal_count_{};
    alignas(hardware_destructive_interference_size) std::atomic<std::uint64_t> steal_tail_{};
    std::vector<Tp, Allocator> ring_buffer_;
  };

  auto advance_get_index(std::size_t& owner, std::size_t owner_index) noexcept -> bool;
  auto advance_steal_index(std::size_t& thief) noexcept -> bool;
  auto advance_put_index(std::size_t& owner, std::uint32_t round) noexcept -> bool;

  // The last block that was owned by the put/get operations
  // this index is only advanced by the owner thread
  alignas(hardware_destructive_interference_size) std::atomic<std::size_t> last_block_{0};
  // start of the thief block index. All steal operations start from here.
  // this index is only advanced by the owner thread when it reclaims a block
  alignas(hardware_destructive_interference_size) std::atomic<std::size_t> start_block_{0};
  std::vector<block_type, allocator_of_t<block_type>> blocks_{};
  std::size_t mask_{};
};

/////////////////////////////////////////////////////////////////////////////
// Implementation of lifo_queue member methods

template <class Tp, class Allocator>
lifo_queue<Tp, Allocator>::lifo_queue(std::size_t num_blocks, std::size_t block_size,
                                      Allocator allocator)
    : blocks_(std::bit_ceil(num_blocks), block_type(block_size, allocator),
              allocator_of_t<block_type>(allocator)),
      mask_(blocks_.size() - 1) {
  blocks_[0].reclaim(0);
}

template <class Tp, class Allocator> auto lifo_queue<Tp, Allocator>::pop_back() noexcept -> Tp {
  std::size_t owner = last_block_.load(std::memory_order_relaxed);
  std::size_t owner_index{};
  do {
    owner_index = owner & mask_;
    block_type& current_block = blocks_[owner_index];
    auto [ec, value] = current_block.get();
    if (ec == lifo_queue_error_code::success) {
      return value;
    }
    if (ec == lifo_queue_error_code::done) {
      return Tp{};
    }
    assert(ec == lifo_queue_error_code::empty);
  } while (advance_get_index(owner, owner_index));
  return Tp{};
}

template <class Tp, class Allocator> auto lifo_queue<Tp, Allocator>::steal_front() noexcept -> Tp {
  std::size_t thief = start_block_.load(std::memory_order_relaxed);
  do {
    std::size_t thief_round = thief / blocks_.size();
    std::size_t thief_index = thief & mask_;
    block_type& block = blocks_[thief_index];
    fetch_result result = block.steal(thief_round);
    while (result.status != lifo_queue_error_code::done) {
      if (result.status == lifo_queue_error_code::success) {
        return result.value;
      }
      if (result.status == lifo_queue_error_code::empty) {
        return Tp{};
      }
      assert(result.status == lifo_queue_error_code::conflict);
      result = block.steal(thief_round);
    }
  } while (advance_steal_index(thief));
  return Tp{};
}

template <class Tp, class Allocator>
auto lifo_queue<Tp, Allocator>::push_back(Tp value) noexcept -> bool {
  std::size_t owner = last_block_.load(std::memory_order_relaxed);
  std::uint32_t round = owner / blocks_.size();
  do {
    std::size_t owner_index = owner & mask_;
    block_type& current_block = blocks_[owner_index];
    auto ec = current_block.put(value);
    if (ec == lifo_queue_error_code::success) {
      return true;
    }
    assert(ec == lifo_queue_error_code::full);
  } while (advance_put_index(owner, round));
  return false;
}

template <class Tp, class Allocator>
template <class Iterator, class Sentinel>
auto lifo_queue<Tp, Allocator>::push_back(Iterator first, Sentinel last) noexcept -> Iterator {
  std::size_t owner = last_block_.load(std::memory_order_relaxed);
  std::size_t owner_round = owner / blocks_.size();
  do {
    std::size_t owner_index = owner & mask_;
    block_type& current_block = blocks_[owner_index];
    first = current_block.bulk_put(first, last);
  } while (first != last && advance_put_index(owner, owner_round));
  return first;
}

template <class Tp, class Allocator>
auto lifo_queue<Tp, Allocator>::block_size() const noexcept -> std::size_t {
  return blocks_[0].block_size();
}

template <class Tp, class Allocator>
auto lifo_queue<Tp, Allocator>::num_blocks() const noexcept -> std::size_t {
  return blocks_.size();
}

template <class Tp, class Allocator>
auto lifo_queue<Tp, Allocator>::advance_get_index(std::size_t& owner,
                                                  std::size_t owner_index) noexcept -> bool {
  std::size_t start = start_block_.load(std::memory_order_relaxed);
  if (start == owner) {
    return false;
  }
  std::size_t predecessor = owner - 1ul;
  std::size_t predecessor_index = predecessor & mask_;
  block_type& previous_block = blocks_[predecessor_index];
  block_type& current_block = blocks_[owner_index];
  current_block.reduce_round();
  previous_block.takeover();
  last_block_.store(predecessor, std::memory_order_relaxed);
  owner = predecessor;
  return true;
}

template <class Tp, class Allocator>
auto lifo_queue<Tp, Allocator>::advance_put_index(std::size_t& owner, std::uint32_t round) noexcept
    -> bool {
  std::size_t next_index = (owner + 1ul) & mask_;
  std::size_t owner_index = owner & mask_;
  if (next_index == owner_index) {
    return false;
  }
  std::uint32_t next_round = next_index == 0 ? round + 1 : round;
  block_type& next_block = blocks_[next_index];
  if (!next_block.is_writable(next_round)) {
    return false;
  }
  std::size_t first = start_block_.load(std::memory_order_relaxed);
  std::size_t first_index = first & mask_;
  if (next_index == first_index) {
    start_block_.store(first_index + 1, std::memory_order_relaxed);
  }
  block_type& current_block = blocks_[owner_index];
  current_block.grant();
  owner += 1;
  next_block.reclaim(next_round);
  last_block_.store(owner, std::memory_order_relaxed);
  return true;
}

template <class Tp, class Allocator>
auto lifo_queue<Tp, Allocator>::advance_steal_index(std::size_t& thief) noexcept -> bool {
  thief += 1;
  std::ptrdiff_t diff = last_block_.load(std::memory_order_relaxed) - thief;
  return diff > 0;
}

/////////////////////////////////////////////////////////////////////////////
// Implementation of lifo_queue::block_type member methods

template <class Tp, class Allocator>
lifo_queue<Tp, Allocator>::block_type::block_type(std::size_t block_size, Allocator allocator)
    : head_{0xFFFF'FFFF'0000'0000 | block_size}, tail_{block_size}, steal_count_{block_size},
      steal_tail_{0xFFFF'FFFF'0000'0000 | block_size}, ring_buffer_(block_size, allocator) {}

template <class Tp, class Allocator>
lifo_queue<Tp, Allocator>::block_type::block_type(const block_type& other)
    : ring_buffer_(other.ring_buffer_) {
  head_.store(other.head_.load(std::memory_order_relaxed), std::memory_order_relaxed);
  tail_.store(other.tail_.load(std::memory_order_relaxed), std::memory_order_relaxed);
  steal_tail_.store(other.steal_tail_.load(std::memory_order_relaxed), std::memory_order_relaxed);
  steal_count_.store(other.steal_count_.load(std::memory_order_relaxed), std::memory_order_relaxed);
}

template <class Tp, class Allocator>
auto lifo_queue<Tp, Allocator>::block_type::operator=(const block_type& other)
    -> lifo_queue<Tp, Allocator>::block_type& {
  head_.store(other.head_.load(std::memory_order_relaxed), std::memory_order_relaxed);
  tail_.store(other.tail_.load(std::memory_order_relaxed), std::memory_order_relaxed);
  steal_tail_.store(other.steal_tail_.load(std::memory_order_relaxed), std::memory_order_relaxed);
  steal_count_.store(other.steal_count_.load(std::memory_order_relaxed), std::memory_order_relaxed);
  ring_buffer_ = other.ring_buffer_;
  return *this;
}

template <class Tp, class Allocator>
lifo_queue<Tp, Allocator>::block_type::block_type(block_type&& other) noexcept {
  head_.store(other.head_.load(std::memory_order_relaxed), std::memory_order_relaxed);
  tail_.store(other.tail_.load(std::memory_order_relaxed), std::memory_order_relaxed);
  steal_tail_.store(other.steal_tail_.load(std::memory_order_relaxed), std::memory_order_relaxed);
  steal_count_.store(other.steal_count_.load(std::memory_order_relaxed), std::memory_order_relaxed);
  ring_buffer_ = std::exchange(std::move(other.ring_buffer_), {});
}

template <class Tp, class Allocator>
auto lifo_queue<Tp, Allocator>::block_type::operator=(block_type&& other) noexcept
    -> lifo_queue<Tp, Allocator>::block_type& {
  head_.store(other.head_.load(std::memory_order_relaxed), std::memory_order_relaxed);
  tail_.store(other.tail_.load(std::memory_order_relaxed), std::memory_order_relaxed);
  steal_tail_.store(other.steal_tail_.load(std::memory_order_relaxed), std::memory_order_relaxed);
  steal_count_.store(other.steal_count_.load(std::memory_order_relaxed), std::memory_order_relaxed);
  ring_buffer_ = std::exchange(std::move(other.ring_buffer_), {});
  return *this;
}

template <class Tp, class Allocator>
auto lifo_queue<Tp, Allocator>::block_type::put(Tp value) noexcept -> lifo_queue_error_code {
  std::uint64_t back = tail_.load(std::memory_order_relaxed);
  if (back < block_size()) [[likely]] {
    ring_buffer_[static_cast<std::size_t>(back)] = static_cast<Tp&&>(value);
    tail_.store(back + 1, std::memory_order_release);
    return lifo_queue_error_code::success;
  }
  return lifo_queue_error_code::full;
}

template <class Tp, class Allocator>
template <class Iterator, class Sentinel>
auto lifo_queue<Tp, Allocator>::block_type::bulk_put(Iterator first, Sentinel last) noexcept
    -> Iterator {
  std::uint64_t back = tail_.load(std::memory_order_relaxed);
  while (first != last && back < block_size()) {
    ring_buffer_[static_cast<std::size_t>(back)] = static_cast<Tp&&>(*first);
    ++back;
    ++first;
  }
  tail_.store(back, std::memory_order_release);
  return first;
}

template <class Tp, class Allocator>
auto lifo_queue<Tp, Allocator>::block_type::get() noexcept -> fetch_result<Tp> {
  std::uint64_t back = tail_.load(std::memory_order_relaxed);
  std::uint64_t back_idx = back & 0xFFFF'FFFFu;
  if (back_idx == 0) [[unlikely]] {
    return {lifo_queue_error_code::empty, nullptr};
  }
  std::uint64_t front = head_.load(std::memory_order_relaxed);
  if (front == back) [[unlikely]] {
    return {lifo_queue_error_code::empty, nullptr};
  }
  Tp value = static_cast<Tp&&>(ring_buffer_[static_cast<std::size_t>(back - 1)]);
  tail_.store(back - 1, std::memory_order_release);
  return {lifo_queue_error_code::success, value};
}

template <class Tp, class Allocator>
auto lifo_queue<Tp, Allocator>::block_type::steal(std::uint32_t thief_round) noexcept
    -> fetch_result<Tp> {
  std::uint64_t spos = steal_tail_.load(std::memory_order_relaxed);
  std::uint64_t sidx = spos & 0xFFFF'FFFFu;
  std::uint64_t round = spos >> 32;
  fetch_result<Tp> result{};
  if (sidx == block_size()) {
    result.status =
        thief_round == round ? lifo_queue_error_code::done : lifo_queue_error_code::empty;
    return result;
  }
  std::uint64_t back = tail_.load(std::memory_order_acquire);
  if (spos == back) {
    result.status = lifo_queue_error_code::empty;
    return result;
  }
  if (!steal_tail_.compare_exchange_strong(spos, spos + 1, std::memory_order_relaxed)) {
    result.status = lifo_queue_error_code::conflict;
    return result;
  }
  result.value = static_cast<Tp&&>(ring_buffer_[static_cast<std::size_t>(spos)]);
  steal_count_.fetch_add(1, std::memory_order_release);
  result.status = lifo_queue_error_code::success;
  return result;
}

template <class Tp, class Allocator>
auto lifo_queue<Tp, Allocator>::block_type::reduce_round() noexcept -> void {
  std::uint64_t steal_tail = steal_tail_.load(std::memory_order_relaxed);
  std::uint32_t round = static_cast<std::uint32_t>(steal_tail >> 32);
  std::uint64_t steal_index = steal_tail & 0xFFFF'FFFFu;
  std::uint64_t new_steal_tail = (static_cast<std::uint64_t>(round - 1) << 32) | steal_index;
  steal_tail_.store(new_steal_tail, std::memory_order_relaxed);
}

template <class Tp, class Allocator>
auto lifo_queue<Tp, Allocator>::block_type::takeover() noexcept -> void {
  std::uint64_t head = head_.load(std::memory_order_relaxed);
  std::uint64_t spos = steal_tail_.exchange(head, std::memory_order_relaxed);
  head_.store(spos, std::memory_order_relaxed);
}

template <class Tp, class Allocator>
auto lifo_queue<Tp, Allocator>::block_type::is_writable(std::uint32_t round) const noexcept
    -> bool {
  std::uint64_t expanded_old_round = static_cast<std::uint64_t>(round - 1) << 32;
  std::uint64_t writeable_spos = expanded_old_round | block_size();
  std::uint64_t spos = steal_tail_.load(std::memory_order_relaxed);
  return spos == writeable_spos;
}

template <class Tp, class Allocator>
auto lifo_queue<Tp, Allocator>::block_type::reclaim(std::uint32_t round) noexcept -> void {
  // we need to wait until all stealers have caught up to steal_tail
  std::uint64_t expected_steal_count_ = head_.load(std::memory_order_relaxed) & 0xFFFF'FFFFu;
  while (steal_count_.load(std::memory_order_acquire) != expected_steal_count_) {
    __builtin_ia32_pause();
  }
  std::uint64_t expanded_round = static_cast<std::uint64_t>(round) << 32;
  head_.store(expanded_round, std::memory_order_relaxed);
  tail_.store(0, std::memory_order_relaxed);
  steal_tail_.store(expanded_round | block_size(), std::memory_order_relaxed);
  steal_count_.store(0, std::memory_order_relaxed);
}

template <class Tp, class Allocator>
auto lifo_queue<Tp, Allocator>::block_type::block_size() const noexcept -> std::size_t {
  return ring_buffer_.size();
}

template <class Tp, class Allocator> void lifo_queue<Tp, Allocator>::block_type::grant() noexcept {
  std::uint64_t block_end = steal_tail_.load(std::memory_order_relaxed);
  std::uint64_t old_head = head_.exchange(block_end, std::memory_order_relaxed);
  steal_tail_.store(old_head, std::memory_order_release);
}
} // namespace cw::bwos
